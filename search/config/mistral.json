{
    "Mistral-7B-v0.3": {
        "n_block": 32,
        "n_layer": 2,
        "layer": ["self_attn", "mlp"],
        "layer_numel": {
            "self_attn": 41943040,
            "mlp": 176160768
        },
        "n_linear": 7,
        "linear": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
        "linear_shape": {
            "self_attn.q_proj": [4096, 4096],
            "self_attn.k_proj": [1024, 4096],
            "self_attn.v_proj": [1024, 4096],
            "self_attn.o_proj": [4096, 4096],
            "mlp.gate_proj": [14336, 4096],
            "mlp.up_proj": [14336, 4096],
            "mlp.down_proj": [4096, 14336]
        },
        "hierarchy": {"self_attn.q_proj": "self_attn", "self_atn.k_proj": "self_attn", "self_attn.v_proj": "self_attn", "self_attn.o_proj": "self_attn", "mlp.gate_proj": "mlp", "mlp.up_proj": "mlp", "mlp.down_proj": "mlp"},
        "model_numel": 6979321856,
        "model": "model", 
        "layers": "model.layers",
        "pre_layer": ["model.embed_tokens"],
        "post_layer": ["model.norm", "lm_head"],
        "k_linear": "self_attn.k_proj",
        "v_linear": "self_attn.v_proj",
        "norm_shape": 4096,
        "n_norm": 65,
        "head_shape": [32768, 4096],
        "pe_shape": [32768, 128],
        "embed_shape": [4096, 32768]
    },

    "Mistral-7B-Instruct-v0.3": {
        "n_block": 32,
        "n_layer": 2,
        "layer": ["self_attn", "mlp"],
        "layer_numel": {
            "self_attn": 41943040,
            "mlp": 176160768
        },
        "n_linear": 7,
        "linear": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
        "linear_shape": {
            "self_attn.q_proj": [4096, 4096],
            "self_attn.k_proj": [1024, 4096],
            "self_attn.v_proj": [1024, 4096],
            "self_attn.o_proj": [4096, 4096],
            "mlp.gate_proj": [14336, 4096],
            "mlp.up_proj": [14336, 4096],
            "mlp.down_proj": [4096, 14336]
        },
        "hierarchy": {"self_attn.q_proj": "self_attn", "self_atn.k_proj": "self_attn", "self_attn.v_proj": "self_attn", "self_attn.o_proj": "self_attn", "mlp.gate_proj": "mlp", "mlp.up_proj": "mlp", "mlp.down_proj": "mlp"},
        "model_numel": 6979321856,
        "model": "model", 
        "layers": "model.layers",
        "pre_layer": ["model.embed_tokens"],
        "post_layer": ["model.norm", "lm_head"],
        "k_linear": "self_attn.k_proj",
        "v_linear": "self_attn.v_proj",
        "vocab_size": 32768,
        "hidden_size": 4096,
        "max_position_embeddings": 32768,
        "head_dim": 128,
        "n_norm": 65
    }
}